
### 深度学习 $\isin$ 机器学习 $\isin$ 人工智能

# Tensor

- 张量即更高维度的数组
- scalar->vector->matrix->**tensor**
- 标量->向量->矩阵->张量

# SGD: Stochastic Gradient Descent

- 梯度下降法即：$\theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta_j}, \alpha是学习率$
- 批量梯度下降：每次更新使用所有样本，全局最优，运算慢
- **随机梯度下降**：每次更新使用一个样本，不一定最优，运算快
- mini-batch梯度下降：每次更新使用一些样本，以上两种的折中

# 评估指标

- 分类问题
    -|实际是|实际否
    -|-|-
    预测是|**TP**: True Positive|FP: False Positive
    预测否|FN: False Negative|**TN**: True Negative
    1. Accuracy: $\frac{TP+TN}{TP+TN+FP+FN}$, 准确率在样本不均衡的时候容易掺水分
    2. Precision: $\frac{TP}{TP+FP}$
    3. **Recall**: $\frac{TP}{TP+FN}$，只关心实际是的部分，宁可错杀一千，绝不放过一个
    4. **F1分数**: $\frac{2 \times Precision \times Recall}{Precision+Recall}$,也即对以上两者指标的调和平均
    5. ROC曲线：基于真正率和假正率，不受样本不平衡影响，越陡越好
    6. AUC：是ROC曲线下面积
- 回归问题
    1. MAE
    2. MSE

# Pre-train

- 预训练主要用于NLP，通过无标注文本训练得到一套模型参数用以初始化，再提供给具体任务细调
- 例如：ELMo、OpenAI GPT、**BERT**

# Machine Learning

- 机器学习就是把现实生活中断问题抽象成数学模型，然后使用机器解决数学问题，再应用于实际评估
- 训练方法
    - 监督学习
    - 无监督学习
    - 强化学习

监督学习|无监督学习
:-:|:-:
目标明确|目标不明确
训练数据带标签|训练数据不带标签
效果容易评估|效果难以量化

- Supervised Learning
    - 监督学习有明确的目标，即人为给定数学模型和已知答案的训练集，机器总结出自己的方法论后，应用于测试集
    - 任务：
        - 回归：预测连续的、具体的数值（回归的目的是预测）
        - 分类：也即离散型的预测
    - 算法：
        算法|类型|简介
        -|-|-
        线性回归|回归|期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）
        逻辑回归|分类|主要解决二分类问题，表达某件事情发生的可能性
        **LDA**|分类|线性判别分析支持多个分类，包括均值、方差
        决策树|分类|基于if-then-else，以信息增益最大化来不断选择特征进行分类
        朴素贝叶斯|分类|假设变量独立，通过预测一个给定的元组属于一个特定类的概率
        **KNN**|分类+回归|K邻近算法通过搜索K个最相似的邻居的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测
        LVQ||用于KNN降低存储内存
        SVM|分类|支持向量机把分类问题转化为寻找分类平面的问题，通过最大化分类边界点距离分类平面的距离来实现分类，只支持二分类
        回归树|回归|决策树的一种，通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益
        **神经网络**|分类+回归|从信息处理角度对人脑神经元网络进行抽象，建立某种简单模型，按不同的连接方式组成不同的网络

- Unsupervised Learning
    - 无监督学习本质是一个统计手段，用以发现数据中的潜在结构
    - 用途：
        1. 发现异常
        2. 用户细分
        3. 推荐系统
    - 算法：
        - 聚类
            - 聚类就是自动分类的方法
            1. **K均值聚类**：就是指定分组数量为K，自动分组
            2. 层次聚类：初始为N个聚类，然后不断把距离最近的向上合并，形成树状结构，一条水平划分即为相应的一种分类
        - 降维
            - 降维就是尽可能保持结构的同时降低复杂度
            1. PCA：主成分分析，相关系数矩阵、特征向量等
            2. SVD：奇异值分解，线性代数中的矩阵分解
- Reinforcement Learning
    - 强化学习不需要数据，而是通过分数刺激奖励来不断学习，主要用于游戏
    - Model-Free：免模型学习更容易获得好的效果
        - Q-Learning
    - Model-Based

- Ensemble Learning
    - 集成学习是一种训练思路，而不是算法，是对已有算法的组合
    - Bagging：即民主
        - e.g. bagging + 决策树 = 随机森林
        - 大家权重都相等，可以并行生成
    - Boosting：即挑选精英
        - e.g. Adaboost 从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器
        - 权重不断调整，只能顺序生成

# Deep Learning
- 深度学习是机器学习最重要的分支，因其突出表现引发了人工智能的第三次浪潮
- 深度学习源于神经网络，是传统神经网络的升级版本，将人工提取特征的过程自动化，也导致了可解释性很差的问题
- **CNN**
    - 卷积神经网络最擅长图片处理，能有效降维大数据量的图片，并且保留图片的特征
    - 卷积层：提取图片特征，卷积核也即基本构图单元
    - 池化层：降维数据，避免过拟合
    - 全连接层：输出结果

- **RNN**
    - 循环神经网络能有效处理序列数据，即相互依赖的顺序数据流，e.g. NLP
    - 有短期记忆的问题，无法处理很长的输入序列
    - **LSTM**：长短期记忆网络只保留重要的信息
    - GRU：对LSTM的简化和调整，节省时间

- **GAN**
    - 对抗生成网络是目前最成功的生成模型，生成器产生图像诱导鉴别者相信，鉴别者会因发现假图片获得奖励
    - 生成器和鉴别者交互训练提升
- RL：深度强化学习

# NLP
- 自然语言处理是人类和机器的翻译官
- 两个核心任务：
    - NLU：自然语言理解
    - NLG：自然语言生成
- Text Mining：文本挖掘就是从数据中寻找有价值的信息
- Representation：文本表示即把文本映射到结构化可计算的向量
    - **One-hot**：过于稀疏，存储效率不高，无法表达词语间关系
    - 整数编码：无法表达词语间关系
    - **Word Embedding**：词嵌入向量维度较低，语义相近的词再向量空间也相近
        - word2vec：18年前很流行，无法解决多义词问题
            - CBOW：词袋模型通过上下文预测当前词
            - Skip-gram：通过当前词预测上下文
        - GloVe：对word2vec的拓展，结合了全局统计和基于上下文的学习
- Encoder-Decoder：把输入转换为向量，再把运算后的向量转换为输出。向量长度固定，信息输入过长会丢失信息
- Seq2Seq：应用于输入序列和输出序列长度可变
- **Attention**：解决了Encoder-Decoder“信息过长，信息丢失”的问题
    - 注意力机制就是**从关注全部到关注重点**，学会提纲挈领
    - 其过程可以概括为**加权求和**
- Transformer